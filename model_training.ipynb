{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae9bfeb6-5c78-447f-a6b3-8fff70ec957a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mean_absolute_error, r2_score\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.io import loadmat\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from PIL import Image\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize,Pad\n",
    "\n",
    "\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n",
    "\n",
    "from deep_learning_project.data_preprocess import SphericalImageRotationDataset,extract_labels_from_filename,create_transformation_matrix,vector_from_rotation,generate_pairs_with_labels,plot_image_pair_with_label\n",
    "from deep_learning_project.Rotation_estimation_paper_architecture import TransformerDecoder,SiameseUNet,ExtremeRotationEstimator,QuaternionLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dac0ad-35eb-4bb9-b4dd-26061c58b0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a68bea-331d-4491-aa93-a2e9b4c27221",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"SphericalImages2\" \n",
    "image_paths = glob.glob(os.path.join(image_dir, \"*.png\"))\n",
    "\n",
    "pairs, labels = generate_pairs_with_labels(image_paths)\n",
    "dataset = SphericalImageRotationDataset(pairs, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6d3e38-7cb9-482c-8966-daeb9b8e4fef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4c5587-3db6-431c-ae55-8ab311aaf29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "# Assuming `dataset` is your complete dataset\n",
    "dataset_size = len(dataset)\n",
    "\n",
    "# Calculate the sizes for train and validation splits\n",
    "val_size = int(0.2 * dataset_size)  # 20% for validation\n",
    "train_size = dataset_size - val_size  # Remaining 80% for training\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders for both splits\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Total Dataset Size: {dataset_size}\")\n",
    "print(f\"Training Dataset Size: {len(train_dataset)}\")\n",
    "print(f\"Validation Dataset Size: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4bea90-1702-40c0-b1ac-a09c7e582b8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba067843-8b60-42d1-b024-bee8b4eb044f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81998ff8-e1bd-44d1-9032-320b6a492e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0473bc88-f1ab-41f0-88ac-0bbf595adb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20): \n",
    "    img1, img2, label = dataset[i]\n",
    "    print(f\"{img1.shape}\")  # (width, height)\n",
    "    print(f\"{img2.shape}\")  # (width, height)\n",
    "    print(f\" {label.shape if isinstance(label, torch.Tensor) else len(label)}\\n\")\n",
    "    \n",
    "    plot_image_pair_with_label(pairs[i][0], pairs[i][1], label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92495346-f121-43f8-b319-706ebe2b83b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ExtremeRotationEstimator()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "loss_function = QuaternionLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27191199-36c7-4f55-a0ff-36e05cbf7794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "# batch_size = 8\n",
    "# img1 = torch.randn(batch_size, 3, 672, 896)  # Example input image 1\n",
    "# img2 = torch.randn(batch_size, 3, 672, 896)  # Example input image 2\n",
    "# rotation_query = torch.randn(batch_size, 3)  # Example ground truth quaternion\n",
    "\n",
    "# # Model\n",
    "# model = ExtremeRotationEstimator()\n",
    "\n",
    "# # Forward Pass\n",
    "# output = model(img1, img2, rotation_query)\n",
    "# print(output.shape)  # Should be (batch_size, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b8328b-a641-4415-9c0a-31803dc68677",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for img1, img2, labels in tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "        img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(img1, img2,labels, None)\n",
    "        # print(outputs.shape)\n",
    "        # print(labels.shape)\n",
    "\n",
    "        loss = loss_function(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3d2c9d-9b5c-4b70-97f8-6b345fbbfa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_model(model, dataloader, loss_function, optimizer, device, num_epochs, accumulation_steps=1, use_autocast=True):\n",
    "    model.to(device)\n",
    "    scaler = GradScaler() if use_autocast else None  # Initialize GradScaler only if using autocast\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for step, (img1, img2, labels) in enumerate(tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")):\n",
    "            img1, img2, labels = img1.to(device, non_blocking=True), img2.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "\n",
    "            # Forward pass with or without mixed precision\n",
    "            if use_autocast:\n",
    "                with autocast():\n",
    "                    outputs = model(img1, img2, labels)\n",
    "                    loss = loss_function(outputs, labels) / accumulation_steps\n",
    "            else:\n",
    "                outputs = model(img1, img2, labels)\n",
    "                loss = loss_function(outputs, labels) / accumulation_steps\n",
    "\n",
    "            # Backward pass\n",
    "            if use_autocast:\n",
    "                scaler.scale(loss).backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            # Gradient accumulation\n",
    "            if (step + 1) % accumulation_steps == 0 or (step + 1) == len(dataloader):\n",
    "                if use_autocast:\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            running_loss += loss.item() * accumulation_steps\n",
    "\n",
    "            # Free unused memory explicitly\n",
    "            del img1, img2, labels, outputs, loss\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(dataloader):.4f}\")\n",
    "        torch.cuda.empty_cache()  # Free GPU memory after each epoch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900c99dc-beae-412e-894f-161a13288e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(\n",
    "    model=model,\n",
    "    dataloader=train_loader,  # Replace with your DataLoader\n",
    "    loss_function=loss_function,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=20,\n",
    "    accumulation_steps=4  # Accumulate gradients over 4 steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261446e4-ed33-4819-a9a8-f5c68f26cdaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d9744f-6250-4162-b9c7-42540eeef998",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"extreme_rotation_estimator_weights_new_loss.pth\")\n",
    "print(\"Model weights saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffcdac4-d172-48cc-87e2-4175872fda7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c436f4da-9e3f-4d4b-830f-a70ed908fb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def calculate_accuracy(predicted, ground_truth, threshold_degrees=10):\n",
    "    \"\"\"\n",
    "    Calculate accuracy based on angular difference between predicted and ground truth quaternions.\n",
    "    \n",
    "    Args:\n",
    "        predicted (torch.Tensor): Predicted quaternions, shape (batch_size, 4).\n",
    "        ground_truth (torch.Tensor): Ground truth quaternions, shape (batch_size, 4).\n",
    "        threshold_degrees (float): Threshold for accuracy in degrees.\n",
    "        \n",
    "    Returns:\n",
    "        float: Accuracy percentage.\n",
    "    \"\"\"\n",
    "    predicted = predicted / torch.norm(predicted, dim=1, keepdim=True)\n",
    "    ground_truth = ground_truth / torch.norm(ground_truth, dim=1, keepdim=True)\n",
    "    \n",
    "    dot_product = torch.sum(predicted * ground_truth, dim=1).clamp(-1.0, 1.0)  # Clamp to avoid numerical issues\n",
    "    angles = 2 * torch.acos(torch.abs(dot_product))  # Angular difference in radians\n",
    "    \n",
    "    angles_degrees = angles * (180.0 / np.pi)\n",
    "    \n",
    "    correct_predictions = (angles_degrees <= threshold_degrees).float()\n",
    "    accuracy = correct_predictions.mean().item() * 100  # Percentage\n",
    "    \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d49346-4540-4d56-9c96-8bd668ee94d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = ExtremeRotationEstimator()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(\"extreme_rotation_estimator_weights.pth\", map_location=device))  # Load weights\n",
    "\n",
    "\n",
    "model.to(device)  # Move to the specified device\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2523809f-0979-4b5a-919d-8b87bbb1cf35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def compute_geodesic_error(predicted_quaternions, ground_truth_quaternions):\n",
    "\n",
    "    predicted_quaternions = predicted_quaternions / torch.norm(predicted_quaternions, dim=1, keepdim=True)\n",
    "    ground_truth_quaternions = ground_truth_quaternions / torch.norm(ground_truth_quaternions, dim=1, keepdim=True)\n",
    "    \n",
    "    # Compute the dot product (cosine of the angle)\n",
    "    dot_product = torch.abs(torch.sum(predicted_quaternions * ground_truth_quaternions, dim=1))\n",
    "    \n",
    "    # Clamp the dot product to avoid numerical issues\n",
    "    dot_product = torch.clamp(dot_product, -1.0, 1.0)\n",
    "    \n",
    "    # Compute the geodesic error (in degrees)\n",
    "    geodesic_error = 2 * torch.acos(dot_product) * (180.0 / torch.pi)\n",
    "    return geodesic_error\n",
    "\n",
    "def evaluate_geodesic_error(model, dataloader, device, threshold_degrees=5):\n",
    "\n",
    "    model.eval()\n",
    "    all_errors = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Extract inputs and ground truth\n",
    "            img1, img2, ground_truth_quaternions = batch\n",
    "            img1, img2, ground_truth_quaternions = img1.to(device), img2.to(device), ground_truth_quaternions.to(device)\n",
    "            \n",
    "            # Generate a placeholder rotation query\n",
    "            rotation_query = torch.zeros(img1.size(0), 3, device=device)\n",
    "            # Forward pass\n",
    "            predicted_quaternions = model(img1, img2, rotation_query)\n",
    "            print(predicted_quaternions,rotation_query)\n",
    "            # Compute geodesic errors for this batch\n",
    "            errors = compute_geodesic_error(predicted_quaternions, ground_truth_quaternions)\n",
    "            all_errors.append(errors)\n",
    "\n",
    "    # Concatenate errors from all batches\n",
    "    all_errors = torch.cat(all_errors).cpu().numpy()\n",
    "    \n",
    "    # Compute metrics\n",
    "    mean_error = all_errors.mean()\n",
    "    median_error = np.median(all_errors)\n",
    "    accuracy = (all_errors < threshold_degrees).mean() * 100  # Percentage\n",
    "    \n",
    "    return {\n",
    "        \"mean_geodesic_error\": mean_error,\n",
    "        \"median_geodesic_error\": median_error,\n",
    "        \"accuracy_below_threshold\": accuracy\n",
    "    }\n",
    "\n",
    "threshold_degrees = 5\n",
    "\n",
    "# # Evaluate on validation/test set\n",
    "# metrics = evaluate_geodesic_error(model, val_loader, device, threshold_degrees=5)\n",
    "# print(f\"Mean Geodesic Error: {metrics['mean_geodesic_error']:.2f}°\")\n",
    "# print(f\"Median Geodesic Error: {metrics['median_geodesic_error']:.2f}°\")\n",
    "# print(f\"{threshold_degrees}° Accuracy: {metrics['accuracy_below_threshold']:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21da7f9f-6609-4a9a-91a8-da4938dd6432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = SphericalImageRotationDataset(pairs[0], labels[0])\n",
    "# dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a1af97-cd0e-4e75-b317-33448d8346e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296b2885-cc79-493e-9229-ea9e67cec0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in dataloader:\n",
    "#     # Extract inputs and ground truth\n",
    "#     img1, img2, ground_truth_quaternions = batch\n",
    "#     img1, img2, ground_truth_quaternions = img1.to(device), img2.to(device), ground_truth_quaternions.to(device)\n",
    "    \n",
    "#     # Generate a placeholder rotation query\n",
    "#     rotation_query = torch.zeros(img1.size(0), 3, device=device)\n",
    "#     # Forward pass\n",
    "#     predicted_quaternions = model(img1, img2, rotation_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f784bd-b5ba-424c-aec6-e9f57303f31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"SphericalImages2\" \n",
    "image_paths = glob.glob(os.path.join(image_dir, \"*.png\"))\n",
    "\n",
    "pairs, labels = generate_pairs_with_labels(image_paths[14:16])\n",
    "dataset = SphericalImageRotationDataset(pairs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49ecdd8-9018-402e-bfa0-87127fdb299c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5335eef9-4867-41e6-8ae1-371bbd838363",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "all_errors = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        img1, img2, ground_truth_quaternions = batch\n",
    "        img1, img2, ground_truth_quaternions = img1.to(device), img2.to(device), ground_truth_quaternions.to(device)\n",
    "        predicted_quaternions = model(img1, img2, ground_truth_quaternions)\n",
    "        print(predicted_quaternions,ground_truth_quaternions)\n",
    "        errors = compute_geodesic_error(predicted_quaternions, ground_truth_quaternions)\n",
    "        all_errors.append(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92c7cca-b389-4fc8-a30c-cdf1301841f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3342faf6-e157-4605-9f16-2c19cd464ac2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
